{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e71df7",
   "metadata": {},
   "source": [
    "# Hardness properties of a polycrystal\n",
    "This notebook and the accompanying code demonstrates how to use the Graph Nets library to learn to predict the hardness map of a polycrystal.\n",
    "\n",
    "The network is trained to predict the load-depth curves of nanoindented grains in steel. \n",
    "\n",
    "After training, the network's prediction ability is illustrated by comparing its output to the true hardness of the material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6340b",
   "metadata": {},
   "source": [
    "# Install dependencies locally\n",
    "\n",
    "If you are running this notebook locally (i.e., not through Colaboratory), you will also need to install a few more dependencies. Run the following on the command line to install the graph networks library, as well as a few other dependencies:\n",
    "\n",
    "```\n",
    "pip install graph_nets matplotlib scipy \"tensorflow>=1.15,<2\" \"dm-sonnet<2\" \"tensorflow_probability<0.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326bab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping installation of Graph Nets library\n"
     ]
    }
   ],
   "source": [
    "install_graph_nets_library = \"No\"  #param [\"Yes\", \"No\"]\n",
    "\n",
    "if install_graph_nets_library.lower() == \"yes\":\n",
    "  print(\"Installing Graph Nets library and dependencies:\")\n",
    "  print(\"Output message from command:\\n\")\n",
    "  !pip install graph_nets \"dm-sonnet<2\" \"tensorflow_probability<0.9\"\n",
    "else:\n",
    "  print(\"Skipping installation of Graph Nets library\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adda139",
   "metadata": {},
   "source": [
    "# load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1414d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Parameters',\n",
       " 'flags',\n",
       " 'gnn library path',\n",
       " 'python libarary path',\n",
       " 'test data files',\n",
       " 'Dislocation Density']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "confParser = configparser.ConfigParser()\n",
    "confParser.read('config_irradiated.ini')\n",
    "confParser.sections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e676854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.15\n",
    "# import tensorflow\n",
    "# tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1425d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(confParser['gnn library path']['gnnLibDir'])\n",
    "sys.path.append(confParser['python libarary path']['pyLibDir'])\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pdb\n",
    "import utility as utl\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = True #--- comment tex stuff!\n",
    "import os\n",
    "import traceback\n",
    "import imp\n",
    "imp.reload(utl)\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.interpolate import Rbf\n",
    "import time\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import functools\n",
    "from typing import Any, Dict, Text,Tuple,Optional\n",
    "\n",
    "\n",
    "import enum\n",
    "import pickle\n",
    "#from absl import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import sonnet as snt\n",
    "\n",
    "from graph_nets import graphs\n",
    "from graph_nets import modules as gn_modules\n",
    "from graph_nets import utils_tf\n",
    "from gnn_graphnet_model import *\n",
    "from graph_nets import utils_np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "SEED =4441666\n",
    "np.random.seed(SEED)\n",
    "#tf.set_random_seed(SEED)\n",
    "tf.compat.v1.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6bdaf",
   "metadata": {},
   "source": [
    "# utility funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa4af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddMissing:\n",
    "    '''\n",
    "    interpolate using k-nearest neighbors\n",
    "    '''\n",
    "    def __init__(self,cords,val,query_index):\n",
    "\n",
    "        self.query_index = query_index.flatten()\n",
    "        self.points    = cords\n",
    "        self.val = val.copy()\n",
    "\n",
    "    def kdTree(self,k):\n",
    "        tree = KDTree( self.points )\n",
    "        dd, ii = tree.query([self.points], k=k)\n",
    "        self.neigh_mat= ii[0] #--- nearest neighbor matrix\n",
    "        \n",
    "    def interpolate(self):\n",
    "        #--- nearest neighbor values\n",
    "        h_mat = np.c_[list(map(lambda x:self.val[x].flatten(),self.neigh_mat))]\n",
    "        assert np.all(list(map(lambda x:np.any(x>0.0),h_mat))), 'increase k!'\n",
    "        #--- filter query points\n",
    "        h_list = list(map(lambda x:x[x>0.0],h_mat[self.query_index]))\n",
    "        assert len(h_list) == h_mat[self.query_index].shape[0]\n",
    "        \n",
    "        #--- average\n",
    "        h_mean = list(map(lambda x: x.mean(), h_list))\n",
    "        query_rows = np.arange(self.query_index.shape[0])[self.query_index]\n",
    "        self.val[query_rows] = np.c_[h_mean]\n",
    "\n",
    "        \n",
    "        \n",
    "def load_test_data(test_data_file_path,test_data_file_path2nd):\n",
    "    # read the csv file and return the text line list.\n",
    "    test_data_row_list = pd.read_csv(test_data_file_path,sep=' ')\n",
    "    test_data_row_list2nd = pd.read_csv(test_data_file_path2nd,sep=' ')\n",
    "    print('open and load data from test_data.csv complete.')\n",
    "    return test_data_row_list, test_data_row_list2nd\n",
    "\n",
    "\n",
    "def FilterDataFrame(test_data_grains,key='id',val=[1,2,3],out='index'):\n",
    "    tmp = pd.DataFrame(np.c_[np.arange(test_data_grains.shape[0]),test_data_grains],columns=[out]+list(test_data_grains.keys()))\n",
    "    return np.c_[tmp.set_index(key,drop=True,append=False).loc[val][out]].flatten().astype(int)\n",
    "\n",
    "    \n",
    "def base_graph( \n",
    "               test_data_file_path, \n",
    "               test_data_file_path2nd,\n",
    "               predictors,\n",
    "                attributes,\n",
    "               logtrans=False\n",
    "              ):\n",
    "    \"\"\"\n",
    "    This here loads the data and forms a graph structure. This should be implemented as it is dataset-dependent.\n",
    "    Output should be:\n",
    "        a dict with  globals (dummy), nodes (nodal data in numpy), edges (edge data), receivers (indices of receiving node in int), senders (int)\n",
    "        train_mask   array of size (n_nodes) with bool or 0/1 indicating training nodes\n",
    "        val_mask     same for validation nodes \n",
    "        test_mask    same for testing nodes\n",
    "        target \t     here the array containing the nodal target data\n",
    "        weight\t     if needed, a weight parameter given for the (training) nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    test_data, test_data2nd = load_test_data(test_data_file_path, \n",
    "                                             test_data_file_path2nd\n",
    "                                            )\n",
    "    test_data_grains, test_data_grains2nd = load_test_data(test_data_file_path, \n",
    "                                         test_data_file_path2nd\n",
    "                                        )\n",
    "\n",
    "    positions = np.c_[test_data.apply(zscore)[attributes]].tolist()\n",
    "    edges = np.c_[test_data2nd[['misOrientationAngle(deg)','boundaryLength(micron)']]].tolist()\n",
    "    \n",
    "\n",
    "    grain_i_indices = FilterDataFrame(test_data_grains,key='#grainID',val=test_data_grains2nd['#grain_i_ID'],out='index')\n",
    "    grain_j_indices = FilterDataFrame(test_data_grains,key='#grainID',val=test_data_grains2nd['grain_j_ID'],out='index')\n",
    "\n",
    "    receivers = list(grain_j_indices)#test_data2nd['grain_j_ID'].astype(int))\n",
    "    senders = list(grain_i_indices) #test_data2nd['#grain_i_ID'].astype(int)) \n",
    "    \n",
    "    #--- target vector\n",
    "    target = None #list(map(lambda x:list(x),predictors))\n",
    "    weight = list(np.ones(test_data.shape[0]))\n",
    "\n",
    "    return {\"globals\": [0.0],  \"nodes\": positions, \"edges\": edges,  \n",
    "            \"receivers\": receivers, \"senders\": senders  },target, weight \n",
    "\n",
    "\n",
    "\n",
    "def create_loss_ops(target_op, output_op, mask, weight=None):\n",
    "    \"\"\"Create supervised loss operations from targets and outputs.\n",
    "\n",
    "    Args:\n",
    "      target_op: The target tf.Tensor.\n",
    "      output_ops: The output graph from the model.\n",
    "\n",
    "    Returns:\n",
    "      loss values (tf.Tensor)\n",
    "    \"\"\"\n",
    "    if weight is None:\n",
    "#         pdb.set_trace()\n",
    "        loss_op = tf.reduce_mean(  (  tf.boolean_mask(output_op.nodes, mask) - tf.boolean_mask(target_op, mask))**2)\n",
    "    else:\n",
    "        loss_op = tf.reduce_mean( tf.boolean_mask(weight, mask)* (  tf.boolean_mask(output_op.nodes, mask) - tf.boolean_mask(target_op, mask))**2)\n",
    "\n",
    "    return loss_op\n",
    "\n",
    "\n",
    "def create_corr_ops(target_op, output_op, mask):\n",
    "    corr_op = tfp.stats.correlation(tf.boolean_mask(target_op, mask), tf.boolean_mask(output_op.nodes, mask))\n",
    "    return corr_op\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f892662c",
   "metadata": {},
   "source": [
    "# Creating graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31193def",
   "metadata": {},
   "source": [
    "## target data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4053630",
   "metadata": {},
   "source": [
    "### parse load depth curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "dd1edcbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LoadDepth:\n",
    "    \n",
    "    def __init__(self,loadLabels):\n",
    "        self.loadLabels = loadLabels\n",
    "        \n",
    "    def inputFiles(self,loadLabel):\n",
    "        return os.listdir('%s/%s'%(confParser['test data files']['load_depth_path'],self.loadLabels[loadLabel]))\n",
    "\n",
    "    def indentLabels(self,loadLabel): \n",
    "        return list(map(lambda x:self.GetIndentLabel(x),self.inputFiles(loadLabel)))\n",
    "\n",
    "    def GetIndentLabel(self,strr):\n",
    "        indxi=strr.find('x')+2\n",
    "        indxf=indxi+2 #strr.find('5000')\n",
    "        return strr[indxi:indxf]\n",
    "    \n",
    "    def loadTimeSeries(self,confParser,loadLabel):\n",
    "        return list(map(lambda x:pd.read_csv('%s/%s/%s'%(confParser['test data files']['load_depth_path'],self.loadLabels[loadLabel],x),\n",
    "                 sep='\\t',index_col=False,names=['Time','Depth','Force']),self.inputFiles(loadLabel)))\n",
    "\n",
    "    def GetGrainId(self,indent,indentLabels,loadID,index):\n",
    "        indentLabel = indentLabels[index]\n",
    "        filtr = np.all([indent['label']==int(indentLabel),indent['#loadID']==loadID],axis=0)\n",
    "        if not np.any(filtr):\n",
    "            'warning: indenter %s not on the plane!'%indentLabel\n",
    "            return indentLabel, np.nan\n",
    "        grainID = indent[filtr]['grainID'].iloc[0]\n",
    "        return indentLabel, grainID\n",
    "\n",
    "    def Write(self,loadID,confParser,path_indented,path):\n",
    "        loadTimeSeries = self.loadTimeSeries(confParser,loadID)\n",
    "        indentLabels = self.indentLabels(loadID)\n",
    "        indent = pd.read_csv('%s/indenter_grainID.txt'%path_indented,sep=' ')\n",
    "        for index in range(len(loadTimeSeries)): #--- file index\n",
    "            ld = loadTimeSeries[ index ] #--- file id\n",
    "            indentLabel, grainID = self.GetGrainId(indent,indentLabels,loadID,index)\n",
    "            print('indent label:%s, grain id:%s'%(indentLabel,grainID))\n",
    "            #\n",
    "            filtr = np.all([ld.Time>0],axis=0)\n",
    "            np.savetxt('%s/loadDepth_GrainID_%s_LoadID%s_IndentLabel_%s.txt'%(path,grainID,loadID,int(indentLabel)),\n",
    "                       np.c_[ld.Depth[filtr], ld.Force[filtr]],\n",
    "                       header='Depth(nm)\\tForce(mN)',\n",
    "                       fmt='%4.3e\\t%4.3e')\n",
    "\n",
    "# os.system('mkdir -p %s/ldGrainID'%confParser['test data files']['load_depth_path'])\n",
    "# load_depth = LoadDepth(\n",
    "#        {0:'10 mN',\n",
    "#         1:'7 mN',\n",
    "#         2:'5 mN',\n",
    "#         3:'4.5 mN',\n",
    "#         4:'4 mN',\n",
    "#         5:'3.5 mN',\n",
    "#         6:'3 mN',\n",
    "#         7:'2.5 mN',\n",
    "#         8:'2 mN',\n",
    "#         9:'1.75 mN',\n",
    "#         10:'1.5 mN',\n",
    "#         11:'1.25 mN'}\n",
    "#         )\n",
    "# for load_id in load_depth.loadLabels:\n",
    "#     print('load_id=',load_id)\n",
    "# #    load_depth.inputFiles(0)\n",
    "# #    load_depth.indentLabels(0)\n",
    "#     #--- read input file\n",
    "# #    load_depth.loadTimeSeries(confParser,0)\n",
    "      #--- write on disc\n",
    "#     load_depth.Write(load_id,\n",
    "#                      confParser,\n",
    "#                      confParser['test data files']['ebsd_path'], #--- indented grain ids\n",
    "#                      confParser['test data files']['load_depth_path']+'/ldGrainID' #--- output\n",
    "#                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "70d4c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finder():\n",
    "    '''\n",
    "    return a list of files in a directory\n",
    "    '''\n",
    "    def __init__(self,path_ld):\n",
    "        self.files = os.listdir(path_ld)\n",
    "\n",
    "    def Get(self,file_index):\n",
    "        return self.files[file_index]\n",
    "    \n",
    "class TestData:\n",
    "    '''\n",
    "    return the feature matrix\n",
    "    '''\n",
    "    ld_curve = {}\n",
    "    load = {}\n",
    "    \n",
    "    def __init__(self,path_ld,path_gb,verbose=False):\n",
    "        self.path_ld = path_ld #--- ld data\n",
    "        self.path_gb = path_gb #--- grain properties\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def is_included(self,xlo,xhi):\n",
    "        '''\n",
    "        return bool: timeseries includes the range (xlo,xhi) \n",
    "        '''\n",
    "        return self.data[:,0].min() <= xlo and self.data[:,0].max() >=  xhi   \n",
    "            \n",
    "    def Parse(self):\n",
    "        if self.verbose:\n",
    "            print('parsing %s'%(self.path_ld))\n",
    "        self.data    = np.loadtxt( self.path_ld ) #--- load curve\n",
    "        self.grains  = pd.read_csv(self.path_gb ,sep=' ')\n",
    "        \n",
    "    def Interp(self,bins):\n",
    "        '''\n",
    "        interpolate on the structured grid bins\n",
    "        '''\n",
    "#        self.xsum = np.interp(bins, self.data[:,0], self.data[:,1], left=None, right=None, period=None)\n",
    "#        self.edges = bins\n",
    "        \n",
    "        self.xsum = self.Rbf(self.data[:,1],len(bins))\n",
    "        self.edges = self.Rbf(self.data[:,0],len(bins))\n",
    "        \n",
    "    def Rbf(self,data,nbins):\n",
    "        x = np.linspace(0, 1, data.shape[0])\n",
    "        d = data \n",
    "        rbfi = Rbf(x, d)\n",
    "        xi = np.linspace(0, 1, nbins)\n",
    "        return rbfi(xi)\n",
    "            \n",
    "    def GetGrainIndex(self):\n",
    "        '''\n",
    "        return grain index and id \n",
    "        '''\n",
    "        GetGrainID = lambda x:x[x.find('ID_')+3:x.find('_LoadID')]\n",
    "#         pdb.set_trace()\n",
    "        GrainID = GetGrainID(self.path_ld)\n",
    "        filtr = self.grains['#grainID']==float(GrainID)\n",
    "        assert np.any(filtr), 'grainID %s not exist!'%GrainID\n",
    "        return self.grains[filtr].index[0]\n",
    "\n",
    "    def Scale(self):\n",
    "        '''\n",
    "        return scaled data \n",
    "        '''\n",
    "        self.data[:,0] /= np.max(self.data[:,1]) #--- scale by fmax\n",
    "        self.data[:,1] /= np.max(self.data[:,1])\n",
    "        \n",
    "    @staticmethod\n",
    "    def ReplaceNanByMean(forces):\n",
    "        mean_force_array = np.mean(forces[~np.any(np.isnan(forces),axis=1)],axis=0) #--- replace missing data by the average\n",
    "        isnan = np.any(np.isnan(forces),axis=1)\n",
    "        \n",
    "        for grain_indx in range(len(forces)):\n",
    "            if isnan[grain_indx]:\n",
    "                forces[grain_indx] = np.copy(mean_force_array)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def Append(GrainIndex,disp,load):\n",
    "        '''\n",
    "        append stress timeseries\n",
    "        '''\n",
    "        TestData.ld_curve.setdefault(GrainIndex,[]).append(disp.copy()) #--- repetative\n",
    "        TestData.load.setdefault(GrainIndex,[]).append(load.copy()) #--- repetative\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def BuildFeature(sdict,ngrains_total,n_ld):\n",
    "    #--- build feature matrix\n",
    "        keys=list(sdict.keys()) #--- indented grains\n",
    "        mat = np.c_[list(map(lambda x:np.mean(np.c_[sdict[x]],axis=0),keys))] #--- matrix\n",
    "#         print(mat.shape)\n",
    "        df = pd.DataFrame(np.c_[keys,mat]) #--- data frame\n",
    "        df.sort_values(0,inplace=True)\n",
    "#         pdb.set_trace()\n",
    "        #--- non-indented grains: inser nans\n",
    "    #    \n",
    "        ngrains_indented = df.shape[0]\n",
    "    #    n_ld = np.arange(xlo,xhi,dx).shape[0]\n",
    "        ngrains = ngrains_total - ngrains_indented\n",
    "        mat_nan = np.ones(ngrains*n_ld).reshape((ngrains,n_ld))*np.nan\n",
    "        #\n",
    "#        keys_nonindented = np.arange(ngrains_total)-keys\n",
    "        keys_nonindented = set(np.arange(ngrains_total))-set(keys)\n",
    "        df_nonindent = pd.DataFrame(np.c_[list(keys_nonindented),mat_nan])\n",
    "        #--- combine\n",
    "        mat_new = np.concatenate([df,df_nonindent],axis=0)\n",
    "        df = pd.DataFrame(np.c_[mat_new]).sort_values(0,inplace=False)\n",
    "    #--- row number\n",
    "#        ids=list(range(ngrains_total))\n",
    "#        list(map(lambda x:ids.remove(x),keys))\n",
    "#        keys = keys + ids\n",
    "\n",
    "#        df = pd.DataFrame(np.c_[keys,mat_new])\n",
    "        return np.c_[df.drop(columns=[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "1ad89b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    !mkdir png\n",
    "    symbols=utl.Symbols()\n",
    "\n",
    "    finder=Finder(confParser['test data files']['load_depth_path']+'/ldGrainID')\n",
    "    display(finder.Get(0))\n",
    "\n",
    "    #--- parse load curve\n",
    "    test_data = TestData(path_ld='%s/%s'%(confParser['test data files']['load_depth_path']+'/ldGrainID',finder.Get(1)),\n",
    "                         path_gb=confParser['test data files']['test_data_file_path'],\n",
    "                        )\n",
    "\n",
    "    test_data.Parse()\n",
    "    #test_data.Scale()\n",
    "    GrainIndex = test_data.GetGrainIndex() #--- could be a nan!\n",
    "\n",
    "    if not eval(confParser['flags']['remote_machine']):\n",
    "        ax=utl.PltErr(test_data.data[:,0],test_data.data[:,1],\n",
    "        #                xlim=(0,20),ylim=(0,0.2),\n",
    "                       attrs=symbols.GetAttrs(count=0,fmt='.',nevery=2),\n",
    "                      Plot=False,\n",
    "                       )\n",
    "        test_data.Interp(bins=np.arange(test_data.data[:,0].min(),test_data.data.max(),0.1))\n",
    "        utl.PltErr(test_data.edges,test_data.xsum,\n",
    "            #                xlim=(0,20),ylim=(0,0.2),\n",
    "                           attrs={'fmt':'-.r'},\n",
    "                            ax=ax,\n",
    "                           xstr='depth(nm)',\n",
    "                           ystr='load(mN)',\n",
    "            #               attrs=symbols.GetAttrs(fmt='-.r'),\n",
    "                           title='png/loadDepth.png'\n",
    "\n",
    "                            )\n",
    "    \n",
    "#main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1a31f",
   "metadata": {},
   "source": [
    "### multiple grains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "7fe4c922",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def GetDuplicateGrains():\n",
    "    s=np.array(list(map(lambda x:len(TestData.ld_curve[x]),TestData.ld_curve.keys())))\n",
    "    mask = s>1\n",
    "    print('grains with more than one indentation data: ',np.array(list(TestData.ld_curve.keys()))[mask].shape)\n",
    "\n",
    "def main():\n",
    "\n",
    "    print('parse load depth curves ...')\n",
    "\n",
    "    test_data_file_path = confParser['test data files']['test_data_file_path']\n",
    "    test_data_file_path2nd = confParser['test data files']['test_data_file_path2nd']\n",
    "    test_data_grains, test_data_grains2nd = load_test_data(test_data_file_path, \n",
    "                                             test_data_file_path2nd\n",
    "                                            )\n",
    "    ngrains = test_data_grains.shape[0]\n",
    "\n",
    "    ax  = utl.PltErr(None,None,Plot=False)\n",
    "\n",
    "    #--- prescribed range for displacements \n",
    "    (xlo,xhi,dx)=(0.0,1.0,0.01)\n",
    "\n",
    "    #--- loop over indented grains\n",
    "    TestData.ld_curve = {}\n",
    "    TestData.load = {}\n",
    "    count_indented = 0\n",
    "    for fp,count in zip(finder.files,range(len(finder.files))):\n",
    "        test_data = TestData(path_ld='%s/%s'%(confParser['test data files']['load_depth_path']+'/ldGrainID',fp),\n",
    "                         path_gb=confParser['test data files']['test_data_file_path'],\n",
    "                         verbose=False,\n",
    "                        )\n",
    "        test_data.Parse()\n",
    "    #    test_data.Scale() #--- scale features\n",
    "    #    if not test_data.is_included(xlo,xhi): #--- ld curve includes this range\n",
    "    #        continue\n",
    "        test_data.Interp(bins=np.arange(xlo,xhi,dx)) #--- interpolate\n",
    "        try:\n",
    "            GrainIndex = test_data.GetGrainIndex() #--- could be a nan!\n",
    "        except:\n",
    "            print('no GrainIndex for ',fp)\n",
    "            continue\n",
    "\n",
    "        if np.any(np.isnan(test_data.data[:,1])):\n",
    "                print('nan in displacements: ',fp)\n",
    "\n",
    "        #--- plot\n",
    "        if not eval(confParser['flags']['remote_machine']):\n",
    "            utl.PltErr(test_data.data[:,0],test_data.data[:,1],\n",
    "                        attrs=symbols.GetAttrs(count=count%7),\n",
    "                       ax=ax,Plot=False,\n",
    "        #               xlim=(0,100),#xhi),# ylim=(0,6),\n",
    "                      )\n",
    "        TestData.Append(GrainIndex,test_data.edges,test_data.xsum) #--- assemble feature matrix: append displacements\n",
    "\n",
    "        count_indented += 1\n",
    "    print('# of indentation exp. is ',count_indented)\n",
    "\n",
    "    GetDuplicateGrains()\n",
    "    #--- predictors are the displacements\n",
    "    predictors = TestData.BuildFeature(TestData.ld_curve, \n",
    "                                       ngrains,\n",
    "                                       np.arange(xlo,xhi,dx).shape[0]\n",
    "                                      )\n",
    "\n",
    "    #---- forces\n",
    "    forces = TestData.BuildFeature(TestData.load, \n",
    "                                       ngrains,\n",
    "                                       np.arange(xlo,xhi,dx).shape[0]\n",
    "                                      )\n",
    "    TestData.ReplaceNanByMean(forces)\n",
    "\n",
    "    print('# of indented grains: ',(~np.any(np.isnan(predictors),axis=1)).sum())\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "9833ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtr = test_data_grains.subBoundaryLength > 0\n",
    "# test_data_grains[filtr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "b9f1f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.scatter(test_data_grains['diameter'],test_data_grains['area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "32a7f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtr = test_data_grains.diameter > 1.0\n",
    "# hist,bin_edges,err = utl.GetPDF(test_data_grains.diameter[filtr],linscale=False,n_per_decade=8) \n",
    "# #hist,bin_edges,err = utl.GetPDF(test_data_grains.diameter[filtr],linscale=True,n_per_decade=16) \n",
    "# utl.PltErr(bin_edges,hist,yerr=err,\n",
    "#           xscale='log',\n",
    "#            yscale='log'\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "0b59ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data_grains.Phi.mean(),test_data_grains.Phi.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977325c9",
   "metadata": {},
   "source": [
    "### missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "6f9d7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddMissing:\n",
    "    '''\n",
    "    interpolate using k-nearest neighbors\n",
    "    '''\n",
    "    def __init__(self,cords,val,query_index,verbose=False):\n",
    "\n",
    "        self.query_index = query_index.flatten()\n",
    "        self.points    = cords\n",
    "        self.val = val.copy()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def kdTree(self,k):\n",
    "        tree = KDTree( self.points )\n",
    "        dd, ii = tree.query([self.points], k=k)\n",
    "        self.neigh_mat= ii[0] #--- nearest neighbor matrix\n",
    "        if self.verbose:\n",
    "            print('neigh_mat.shape:',self.neigh_mat.shape)\n",
    "        \n",
    "    def interpolate(self):\n",
    "        #--- nearest neighbor values\n",
    "        h_mat = np.c_[list(map(lambda x:self.val[x],self.neigh_mat))]\n",
    "        if self.verbose:\n",
    "            print('h_mat.shape:',h_mat.shape)\n",
    "        assert np.all(list(map(lambda x:np.any(~np.isnan(x)),h_mat))), 'increase k!'\n",
    "        #--- filter query points\n",
    "        h_list = list(map(lambda x:x[np.all(~np.isnan(x),axis=1)],h_mat[self.query_index]))\n",
    "        assert len(h_list) == h_mat[self.query_index].shape[0]\n",
    "        \n",
    "        #--- average\n",
    "        h_mean = list(map(lambda x: np.mean(x,axis=0), h_list))\n",
    "        if self.verbose:\n",
    "            print('h_mean.shape:',h_mean[0].shape)\n",
    "        query_rows = np.arange(self.query_index.shape[0])[self.query_index]\n",
    "        self.val[query_rows] = np.c_[h_mean]\n",
    "\n",
    "        \n",
    "\n",
    "# interp0 = AddMissing(np.c_[test_data_grains[['x','y']]],\n",
    "#                      predictors,#np.c_[df.drop(columns=[0])],\n",
    "#                      np.all(np.isnan(predictors),axis=1),\n",
    "#                      verbose=True,\n",
    "#                            )\n",
    "\n",
    "# interp0.kdTree(64) #--- nearest neighbors\n",
    "# interp0.interpolate()\n",
    "# predictors = interp0.val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e8e2a",
   "metadata": {},
   "source": [
    "## build graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911df37",
   "metadata": {},
   "source": [
    "### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "f5aa9873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class train_test_split:\n",
    "    '''\n",
    "    Split arrays or matrices into random train and test subsets.\n",
    "    '''\n",
    "    def __init__(self,mask,\n",
    "                 random_state=128,\n",
    "                test_size=.3, train_size=.7):\n",
    "        self.mask = mask\n",
    "        self.random_state = random_state\n",
    "        self.test_size=test_size\n",
    "        self.train_size=train_size\n",
    "        \n",
    "    def train_test_split(self):\n",
    "        '''\n",
    "        random train-test split\n",
    "        '''\n",
    "        n=self.mask.shape[0]\n",
    "        indented_indices = np.arange(n)[self.mask]\n",
    "        n_indented = indented_indices.shape[0]\n",
    "        np.random.seed(self.random_state)\n",
    "        np.random.shuffle(indented_indices)\n",
    "        #\n",
    "        m=int(self.test_size*n_indented)\n",
    "        test_set_indices = indented_indices[:m] #--- test set indices\n",
    "        test_set=np.zeros(n,dtype=bool)\n",
    "        test_set[test_set_indices]=True #--- assign True to test set oindices\n",
    "\n",
    "        #\n",
    "        m=int(self.train_size*n_indented)\n",
    "        train_set_indices=indented_indices[n_indented-m:n_indented]\n",
    "        train_set=np.zeros(n,dtype=bool)\n",
    "        train_set[train_set_indices]=True #--- assign True to test set oindices\n",
    "\n",
    "#        return train_set, test_set\n",
    "        self.train_mask = train_set\n",
    "        self.test_mask = test_set\n",
    "    \n",
    "    def train_test_split_structured(self,xy):\n",
    "        '''\n",
    "        structured split\n",
    "        '''\n",
    "        x=xy[:,0]\n",
    "        y=xy[:,1]\n",
    "        xlo, xhi = x.min(), x.max()\n",
    "        ylo, yhi = y.min(), y.max()\n",
    "        x_copy = np.array(x.copy() - xlo)\n",
    "        y_copy = np.array(y.copy() - ylo)\n",
    "\n",
    "        self.train_mask = np.all([x_copy < self.train_size * (xhi-xlo), self.mask],axis=0)\n",
    "        self.test_mask = np.all([~self.train_mask, self.mask],axis=0)\n",
    "    \n",
    "    def train_test_split_cv(self,cv=3):\n",
    "        '''\n",
    "        Split arrays or matrices into *random* train and test subsets for cross validation.\n",
    "        '''\n",
    "        assert cv > 1, '# of partitions must be greater than 2'\n",
    "        n=self.mask.shape[0]\n",
    "        indented_indices = np.arange(n)[self.mask]\n",
    "        n_indented = indented_indices.shape[0]\n",
    "        np.random.seed(self.random_state)\n",
    "        np.random.shuffle(indented_indices)\n",
    "        m=int(n_indented/cv)\n",
    "        test_set = {}\n",
    "        for i in range(cv-1):\n",
    "            test_set[i] = indented_indices[i*m:(i+1)*m] #--- test set indices\n",
    "        test_set[i+1] = indented_indices[(i+1)*m:n_indented]\n",
    "        assert n_indented == np.sum(list(map(lambda x:test_set[x].shape[0],test_set.keys())))\n",
    "        #\n",
    "        mask_dic = {}\n",
    "        for i in range(cv):\n",
    "            tmp_test=np.zeros(n,dtype=bool)\n",
    "            tmp_test[test_set[i]]=True #--- assign True to test set oindices\n",
    "            tmp_train = np.all([self.mask,~tmp_test],axis=0)\n",
    "            assert not np.any(np.all([tmp_train,tmp_test],axis=0))\n",
    "            mask_dic[i]={}\n",
    "            mask_dic[i]['test'] = np.copy(tmp_test)\n",
    "            mask_dic[i]['train'] = np.copy(tmp_train)\n",
    "\n",
    "        self.mask_dic = mask_dic\n",
    "    \n",
    "    def train_test_split_cv_structured(self,xy,cv):\n",
    "        '''\n",
    "        Split arrays or matrices into *structured* train and test subsets for cross validation.\n",
    "        '''\n",
    "        m=cv[0]\n",
    "        n=cv[1]\n",
    "        x=xy[:,0]\n",
    "        y=xy[:,1]\n",
    "        xlo, xhi = x.min()-1e-6, x.max()+1e-6\n",
    "        assert xhi > xlo\n",
    "        ylo, yhi = y.min()-1e-6, y.max()+1e-6\n",
    "        assert yhi > ylo\n",
    "        x_copy = np.array(x.copy() - xlo)\n",
    "        y_copy = np.array(y.copy() - ylo)\n",
    "        lx = xhi-xlo\n",
    "        ly = yhi-ylo\n",
    "        #dy = ly / m\n",
    "        #dx = lx / n\n",
    "        df=pd.DataFrame(np.c_[list(map(int,m*y_copy / ly)),list(map(int,n*x_copy / lx))],columns=['row','col'])\n",
    "        groups=df.groupby(by=['row','col']).groups\n",
    "\n",
    "        mask_dic={}\n",
    "        for igroup, count in zip(groups.keys(),range(m*n)):\n",
    "            mask_dic[count]={}\n",
    "            tmp = np.zeros(x.shape[0],dtype=bool)\n",
    "            true_indices = groups[ igroup ]\n",
    "            tmp[true_indices] = True\n",
    "            tmp = np.all([tmp, self.mask],axis=0)\n",
    "            mask_dic[count]['test'] = np.copy(tmp)\n",
    "            mask_dic[count]['train'] = np.all([~tmp, self.mask],axis=0)\n",
    "        #    mask_dic[count]['train'] = np.copy(tmp_train)\n",
    "        self.mask_dic = mask_dic\n",
    "    \n",
    "    def Plot(self,test_data_grains,train_mask,test_mask):\n",
    "        ax=utl.PltErr(test_data_grains[train_mask]['x'],test_data_grains[train_mask]['y'],\n",
    "                  attrs={'fmt':'.'},\n",
    "                 Plot=False\n",
    "                 )\n",
    "\n",
    "        utl.PltErr(test_data_grains[test_mask]['x'],test_data_grains[test_mask]['y'],\n",
    "                      attrs={'fmt':'.','color':'red'},\n",
    "                   ax=ax\n",
    "                     )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "7d569ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('test/train split ...')\n",
    "\n",
    "    mask = np.all(~np.isnan(predictors),axis=1) #--- indented grains\n",
    "    #\n",
    "    tsp = train_test_split(mask,\n",
    "                     random_state=eval(confParser['Parameters']['seed']),\n",
    "                    test_size=.3, train_size=.7)\n",
    "\n",
    "    #--- train-test split (random)\n",
    "    print('train-test split (random)')\n",
    "    tsp.train_test_split()\n",
    "    if not eval(confParser['flags']['remote_machine']):\n",
    "        tsp.Plot(test_data_grains,\n",
    "                 tsp.train_mask,\n",
    "                 tsp.test_mask)\n",
    "\n",
    "    #--- cross validate\n",
    "    print('cv (random)')\n",
    "    tsp.train_test_split_cv(eval(confParser['Parameters']['n_cross_val']))\n",
    "    if not eval(confParser['flags']['remote_machine']):\n",
    "        list(map(lambda x:tsp.Plot(test_data_grains,\n",
    "                               tsp.mask_dic[x]['train'],\n",
    "                               tsp.mask_dic[x]['test']),tsp.mask_dic.keys()))\n",
    "\n",
    "    #--- train-test split (structured)\n",
    "    print('train-test split (structured)')\n",
    "    tsp.train_test_split_structured(np.c_[test_data_grains[['x','y']]])\n",
    "    if not eval(confParser['flags']['remote_machine']):\n",
    "        tsp.Plot(test_data_grains,\n",
    "                 tsp.train_mask,\n",
    "                 tsp.test_mask)\n",
    "\n",
    "    #--- cross validate\n",
    "    print('cv (structured)')\n",
    "    n_cv = eval(confParser['Parameters']['n_cross_val'])\n",
    "    tsp.train_test_split_cv_structured(np.c_[test_data_grains[['x','y']]],\n",
    "                                                         (int(n_cv**.5),int(n_cv**.5))\n",
    "                                                         )\n",
    "    if not eval(confParser['flags']['remote_machine']):\n",
    "        list(map(lambda x:tsp.Plot(test_data_grains,\n",
    "                                   tsp.mask_dic[x]['train'],\n",
    "                                   tsp.mask_dic[x]['test']),tsp.mask_dic.keys()))\n",
    "        \n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "dcb5929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSubset(mask,fraction):\n",
    "    n=mask.shape[0]\n",
    "    indented_indices = np.arange(n)[mask]\n",
    "    n_indented = indented_indices.shape[0]\n",
    "    np.random.seed(128)\n",
    "    np.random.shuffle(indented_indices)\n",
    "    m=int(fraction*n_indented)\n",
    "    test_set = indented_indices[:m] #--- test set indices\n",
    "    #\n",
    "    tmp_test=np.zeros(n,dtype=bool)\n",
    "    tmp_test[test_set]=True #--- assign True to test set oindices\n",
    "    return tmp_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "d82e1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BadGrains(mask,slist):\n",
    "    for i in slist:\n",
    "        mask[i]=False\n",
    "#BadGrains(train_mask,[0,21,43,50,83,86,100,109,112])\n",
    "#BadGrains(test_mask, [0,21,43,50,83,86,100,109,112])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3a675",
   "metadata": {},
   "source": [
    "### input graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "a56bf10c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open and load data from test_data.csv complete.\n",
      "open and load data from test_data.csv complete.\n",
      "dict_keys(['globals', 'nodes', 'edges', 'receivers', 'senders'])\n",
      "globals\n",
      "nodes\n",
      "edges\n",
      "receivers\n",
      "senders\n",
      "nodes matrix shape: (61, 12)\n",
      "edge matrix shape: (147, 2)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    #tf.set_random_seed(1234)\n",
    "    #tf.get_default_graph().finalize() # something everybody tends to forget\n",
    "\n",
    "\n",
    "    pure_lab='' \n",
    "    data_split = None\n",
    "    data_split_lab=''\n",
    "    #\n",
    "\n",
    "\n",
    "    #--- path for csv data files\n",
    "    test_data_file_path=confParser['test data files']['test_data_file_path']\n",
    "    test_data_file_path2nd=confParser['test data files']['test_data_file_path2nd']\n",
    "\n",
    "\n",
    "    \n",
    "    #--- graph structure  \n",
    "    static_graph_tr,\\\n",
    "    target_nodes_np, weight_np = base_graph(test_data_file_path, \n",
    "                                            test_data_file_path2nd,\n",
    "                                            None,\n",
    "                                            confParser['Parameters']['attributes'].split(), #--- attributes\n",
    "                                            logtrans=data_split)\n",
    "    print(static_graph_tr.keys())\n",
    "    for k in static_graph_tr.keys():\n",
    "        try:\n",
    "            print(k, static_graph_tr[k].shape)\n",
    "        except AttributeError:\n",
    "            print(k)\n",
    "\n",
    "    #--- graph without edges\n",
    "    # if eval(confParser['flags']['without_edges']): \n",
    "    # #     static_graph_tr.pop('edges')\n",
    "    # #     static_graph_tr.pop('receivers')\n",
    "    # #     static_graph_tr.pop('senders')\n",
    "    #     static_graph_tr['edges']=[static_graph_tr['edges'][0]]\n",
    "    #     static_graph_tr['receivers']=[static_graph_tr['receivers'][0]]\n",
    "    #     static_graph_tr['senders']=[static_graph_tr['senders'][0]]\n",
    "\n",
    "    input_graph = utils_tf.data_dicts_to_graphs_tuple([static_graph_tr])\n",
    "    graphs_tuple = utils_np.data_dicts_to_graphs_tuple([static_graph_tr])\n",
    "    #print(input_graph)\n",
    "    print('nodes matrix shape:',np.array(static_graph_tr['nodes']).shape)\n",
    "    if not eval(confParser['flags']['without_edges']): \n",
    "        print('edge matrix shape:',np.array(static_graph_tr['edges']).shape)\n",
    "        \n",
    "    return static_graph_tr\n",
    "\n",
    "static_graph_tr = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce4966",
   "metadata": {},
   "source": [
    "## Visualize \n",
    "### euclidean space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "d41113ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class interpolate:\n",
    "    '''\n",
    "    interpolate using k-nearest neighbors\n",
    "    '''\n",
    "    def __init__(self,cords,val,query_points):\n",
    "\n",
    "        self.points    = cords\n",
    "        self.query_points = query_points\n",
    "        self.val = val.copy()\n",
    "\n",
    "    def kdTree(self):\n",
    "        tree = KDTree( self.points )\n",
    "        dd, ii = tree.query([self.query_points], k=1)\n",
    "        self.neigh_mat= ii[0] #--- nearest neighbor matrix\n",
    "        \n",
    "        \n",
    "    def interpolate(self):        \n",
    "        return self.val[self.neigh_mat.flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065afcb6",
   "metadata": {},
   "source": [
    "### nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "4cffb288",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_cmap(n):\n",
    "    colors = [(1,1,1)] + [(np.random.random(),np.random.random(),np.random.random()) for i in range(n-1)]\n",
    "    new_map = matplotlib.colors.LinearSegmentedColormap.from_list('new_map', colors, N=n)\n",
    "    return new_map\n",
    "\n",
    "\n",
    "def main():\n",
    "    if  eval(confParser['flags']['remote_machine']) or eval(confParser['flags']['without_edges']):\n",
    "        return\n",
    "    \n",
    "    test_data_grains, test_data_grains2nd = load_test_data(confParser['test data files']['test_data_file_path'], \n",
    "                                          confParser['test data files']['test_data_file_path2nd']\n",
    "                                         )\n",
    "\n",
    "    #--- boundary segments\n",
    "    data = np.loadtxt('%s/boundaryPixels.txt'%confParser['test data files']['ebsd_path'])\n",
    "    df_boundary = pd.DataFrame(data,columns='grainID1 grainID2 x y phi1 Phi phi2 angle'.split())\n",
    "    [xlo,xhi]=[df_boundary.x.min(),df_boundary.x.max()]\n",
    "    [ylo,yhi]=[df_boundary.y.min(),df_boundary.y.max()]\n",
    "    \n",
    "    \n",
    "    new_map = get_cmap(test_data_grains.shape[0])\n",
    "    #--- plott grains\n",
    "    id_matrix = np.loadtxt('%s/id_matrix.txt'%confParser['test data files']['ebsd_path'],\n",
    "              )\n",
    "    n=id_matrix.T.shape[1]\n",
    "    val=np.array([id_matrix.T[:,(n-1)-j] for j in range(n)])\n",
    "    points=np.c_[test_data_grains[['x','y']]]\n",
    "#     [xlo,xhi]=[points[:,0].min(),points[:,0].max()]\n",
    "#     [ylo,yhi]=[points[:,1].min(),points[:,1].max()]\n",
    "    plot_bitmap = True\n",
    "    if not plot_bitmap:\n",
    "        val[:,:]=0\n",
    "    plt.imshow(val.T,\n",
    "            #            id_matrix,\n",
    "                        origin='lower',\n",
    "                        extent = (yhi,ylo,xlo,xhi),\n",
    "            #               extent = (0,-100,-100,0),\n",
    "                        cmap=new_map\n",
    "                      )\n",
    "\n",
    "    #--- plot centers\n",
    "    plot_centers=True\n",
    "    pixel_cutoff=0\n",
    "    filtr = test_data_grains['grainSize']>pixel_cutoff\n",
    "    points=np.c_[test_data_grains[filtr][['x','y']]]\n",
    "    xt=points[:,0] \n",
    "    yt=points[:,1]\n",
    "    ids = np.c_[test_data_grains[filtr]['#grainID']].astype(str).flatten()\n",
    "\n",
    "    #--- plot edges\n",
    "    singlePixel = test_data_grains[test_data_grains['grainSize']<=pixel_cutoff].index\n",
    "    for i,j in zip(static_graph_tr['senders'],static_graph_tr['receivers']):\n",
    "        if i in singlePixel or j in singlePixel:\n",
    "            continue\n",
    "        x=[points[i][0],points[j][0]] \n",
    "        y=[points[i][1],points[j][1]]\n",
    "#         print(y,x)\n",
    "        if plot_centers:\n",
    "            plt.plot(y,x,\n",
    "                       '-',color='black',\n",
    "                     lw=.3,\n",
    "                      )\n",
    "\n",
    "    if plot_centers:\n",
    "        plt.plot(yt,\n",
    "                 xt,\n",
    "                 '.',color='black',\n",
    "                 markersize=4,\n",
    "                )\n",
    "#         print(ids)\n",
    "        for item, text in zip(points,ids):\n",
    "            plt.text(item[1],item[0],text)\n",
    "        \n",
    "        \n",
    "    #--- boundary segments\n",
    "    plt.plot(df_boundary.y,\n",
    "             df_boundary.x,\n",
    "             '.',color='black',\n",
    "             markersize=.1,\n",
    "            )\n",
    "    \n",
    "\n",
    "    #--- range\n",
    "#     l=h=30\n",
    "#     xc=-40;yc=-60\n",
    "#     plt.xlim(xc+l/2,xc-l/2)\n",
    "#     plt.ylim(yc-h/2,yc+h/2)\n",
    "    plt.savefig('png/gnnMagnified.png',bbox_inches='tight',pad_inches=0.0,dpi=600)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c2b48",
   "metadata": {},
   "source": [
    "## Dislocation Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "ba54f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DislocationDensity:\n",
    "    def __init__(self,id_matrix, xv, yv, **kwargs):\n",
    "    \n",
    "        self.id_matrix = id_matrix #--- grain-id matrix\n",
    "        self.xv,self.yv = xv, yv        \n",
    "        \n",
    "        cart_indices = 'alpha11 alpha12 alpha13 alpha21 alpha22 alpha23 alpha31 alpha32 alpha33'.split()\n",
    "        \n",
    "        for key in kwargs:\n",
    "            val = kwargs[ key ]\n",
    "            setattr(self, key,val)\n",
    "            cart_indices.remove(key) #--- set remaining indices to zero\n",
    "        \n",
    "        for indices in cart_indices:\n",
    "            val = np.zeros(id_matrix.size).reshape(id_matrix.shape)\n",
    "            key = indices\n",
    "            setattr(self, key,val)\n",
    "\n",
    "        #--- tensor norm \n",
    "        self.value = np.nansum([\n",
    "            self.alpha11 * self.alpha11,\n",
    "            self.alpha12 * self.alpha12,\n",
    "            self.alpha13 * self.alpha13,\n",
    "            self.alpha21 * self.alpha21,\n",
    "            self.alpha22 * self.alpha22,\n",
    "            self.alpha23 * self.alpha23,\n",
    "            self.alpha31 * self.alpha31,\n",
    "            self.alpha32 * self.alpha32,\n",
    "            self.alpha33 * self.alpha33\n",
    "                               ], axis=0) ** 0.5\n",
    "        \n",
    "\n",
    "                \n",
    "    def PlotDensityMap(self,GrainID, \n",
    "                       cords,\n",
    "                       boundary_segments = False,\n",
    "                       grid = False,\n",
    "                       **kwargs):\n",
    "                \n",
    "        ylo = self.yv.min()\n",
    "        yhi = self.yv.max()\n",
    "        xlo = self.xv.min()\n",
    "        xhi = self.xv.max()\n",
    "        #\n",
    "        ax=utl.PltBitmap(self.value,\n",
    "                   xlim = (xlo,xhi),ylim = (ylo,yhi),\n",
    "                     zscore=True,\n",
    "                     cmap='seismic',\n",
    "                    mask = self.value == np.nan,\n",
    "                     **kwargs\n",
    "                  )\n",
    "\n",
    "        #--- boundary segments\n",
    "        if boundary_segments:\n",
    "            x = self.cordxyBoundary.real\n",
    "            y = self.cordxyBoundary.imag\n",
    "            ax.plot(x,y,'.',color='brown',markersize=2)\n",
    "\n",
    "        #--- center\n",
    "#         ax.plot([cords[0]],[cords[1]],color='black',marker='.',markersize=20)\n",
    "\n",
    "        #--- grid\n",
    "        if grid:\n",
    "            ax.plot(self.pixel_cords_inside.real,\n",
    "                     self.pixel_cords_inside.imag,'.',color='yellow',markersize=2)\n",
    "\n",
    "        plt.savefig(kwargs['title'],bbox_inches='tight',pad_inches=0.0)\n",
    "        \n",
    "    def Mask(self,GrainID):\n",
    "        filtr = self.id_matrix == GrainID\n",
    "        rows = np.any(filtr,axis=1)\n",
    "        cols = np.any(filtr,axis=0)\n",
    "        self.pixel_cords_inside = self.xv[filtr]+1j*self.yv[filtr]\n",
    "        self.pixel_value_inside = self.value[filtr]\n",
    "\n",
    "        #--- sub matrix representing the given grain\n",
    "        self.id_matrix = (self.id_matrix[rows])[:,cols]\n",
    "        self.value = (self.value[rows])[:,cols]\n",
    "        self.xv = (self.xv[rows])[:,cols]\n",
    "        self.yv = (self.yv[rows])[:,cols]\n",
    "\n",
    "    \n",
    "\n",
    "    def DistanceMatrix(self,center):\n",
    "#         xlin = np.linspace(self.xlo,self.xhi,self.id_matrix.shape[1])\n",
    "#         ylin = np.linspace(self.ylo,self.yhi,self.id_matrix.shape[0])\n",
    "#         xv, yv = np.meshgrid( xlin, ylin )\n",
    "\n",
    "#         self.rdist = (xv-center[0])+1j*(yv-center[1]) self.misOrientationBoundary\n",
    "        self.nearestBoundarySeg  = np.array(list(map(lambda x:\n",
    "                              np.argmin(np.abs(self.cordxyBoundary-x)),self.pixel_cords_inside)))\n",
    "\n",
    "        self.MinDistance    = np.array(list(map(lambda x:np.abs(self.cordxyBoundary[x[1]]-x[0]),\n",
    "                                                zip(self.pixel_cords_inside,self.nearestBoundarySeg))))\n",
    "        \n",
    "        \n",
    "        self.MinOrientation = np.array(list(map(lambda x:self.misOrientationBoundary[x],\n",
    "                                                self.nearestBoundarySeg)))\n",
    "    \n",
    "    def DensityProfile(self, indx, \n",
    "                       fout, plot=True, scale = False, **kwargs):\n",
    "        rwjs = utl.ReadWriteJson(append=True)\n",
    "        data = {'GrainIndex'    : indx,\n",
    "                'Distance'      : self.MinDistance if not scale else  self.MinDistance / np.max( self.MinDistance ),\n",
    "                'Misorientation': self.MinOrientation,\n",
    "                'Density'       : self.pixel_value_inside\n",
    "               }\n",
    "        rwjs.Write( [ data ], fout )\n",
    "\n",
    "\n",
    "#         with open(fout,'w') as fout:\n",
    "#             np.savetxt(fout,np.c_[self.MinDistance/np.max(self.MinDistance),\n",
    "#                                   self.pixel_value_inside],header='r rho')\n",
    "        if plot:\n",
    "            utl.PltErr(self.MinDistance,self.pixel_value_inside,\n",
    "                       **kwargs\n",
    "                      )\n",
    "        \n",
    "    def PrintGrainGrainDensity(self,igrain,cords,neighbors_i_indices):\n",
    "        centeri = cords[ igrain ]\n",
    "        \n",
    "        self.rx = self.rdist.real \n",
    "        self.ry = self.rdist.imag \n",
    "        rwj = utl.ReadWriteJson()\n",
    "        data = []\n",
    "        for neighbor_j in neighbors_i_indices:\n",
    "                    #--- neighbors ij density\n",
    "            centerj = cords[ neighbor_j ]\n",
    "            rij = centerj - centeri\n",
    "            mag = np.sum(rij*rij)**0.5\n",
    "            rij /= mag\n",
    "        \n",
    "            #--- project\n",
    "            r_projected = self.rx * rij[ 0 ] + self.ry * rij[ 1 ]\n",
    "\n",
    "        #--- print \n",
    "            sdict = {}\n",
    "            sdict['igrain'] = float(igrain)\n",
    "            sdict['jgrain'] = float(neighbor_j)\n",
    "            sdict['rij'] = r_projected.flatten()\n",
    "            sdict['rhoij'] = np.abs(self.value).flatten()\n",
    "            data.append( sdict )\n",
    "#             pdb.set_trace()\n",
    "        rwj.Write(data, 'png/GrainGrainDensity_igrain%s.json'%igrain)\n",
    "            \n",
    "    def GrainGrainDensity(self,igrain,jgrain):\n",
    "        rwj = utl.ReadWriteJson()\n",
    "        try:\n",
    "            idata = rwj.Read('png/GrainGrainDensity_igrain%s.json'%igrain)\n",
    "            jdata = rwj.Read('png/GrainGrainDensity_igrain%s.json'%jgrain)\n",
    "        except:\n",
    "            print('file not found!')\n",
    "            return\n",
    "\n",
    "        for items in idata:\n",
    "            assert int(items['igrain']) == igrain\n",
    "            if int(items['jgrain']) == jgrain:\n",
    "                rij = items['rij']\n",
    "                rhoij = items['rhoij']\n",
    "                break    \n",
    "                \n",
    "        for items in jdata:\n",
    "            assert int(items['igrain']) == jgrain\n",
    "            if int(items['jgrain']) == igrain:\n",
    "                rji = items['rij']\n",
    "                rhoji = items['rhoij']\n",
    "                break\n",
    "                \n",
    "        #--- plot\n",
    "        xdata = np.concatenate([rij-np.max(rij),np.max(rji)-rji])\n",
    "        ydata = np.concatenate([np.abs(rhoij),np.abs(rhoji)])\n",
    "        utl.PltErr(xdata,ydata,\n",
    "           xscale='linear',yscale='log',\n",
    "           attrs={'fmt':'x','alpha':1.0},\n",
    "           xstr='Distance from the center (micron)', ystr=r'$|\\alpha_{12}|$',\n",
    "#           **kwargs\n",
    "          )\n",
    "\n",
    "    def ParseBoundarySegments(self,GrainID, data, pairs ):\n",
    "        self.boundarySegments       = pd.DataFrame(data,columns='grainID1 grainID2 x y phi1 Phi phi2 angle'.split())\n",
    "        filtr                       = np.any([self.boundarySegments.grainID1 == GrainID, \n",
    "                                              self.boundarySegments.grainID2 == GrainID ], axis = 0 )\n",
    "        self.cordxyBoundary         = np.array(list(map(lambda x:x[0]+x[1]*1j,np.c_[self.boundarySegments[filtr]['x y'.split()]])))\n",
    "#        self.misOrientationBoundary = np.c_[self.boundarySegments[filtr]['angle']]\n",
    "\n",
    "        df                          = self.boundarySegments[ filtr ]\n",
    "        filtr2nd                    = np.c_[df['grainID1 grainID2'.split()] == GrainID]\n",
    "        self.neighborIdBoundary     = np.c_[df['grainID1 grainID2'.split()]][~filtr2nd]\n",
    "\n",
    "        assert GrainID not in self.neighborIdBoundary\n",
    "        assert self.neighborIdBoundary.shape[0] == self.cordxyBoundary.shape[0]\n",
    " \n",
    "        \n",
    "        #--- misOrientationBoundary\n",
    "        neighbors = DislocationDensity.GetNeighbors(pairs,GrainID)\n",
    "        self.misOrientationBoundary =\\\n",
    "        list(map(lambda x: neighbors[ neighbors.grain_j_ID == x]['misOrientationAngle(deg)'].iloc[ 0 ]if x != 0.0 else np.nan,\n",
    "        self.neighborIdBoundary))\n",
    "\n",
    "\n",
    "    @staticmethod    \n",
    "    def GetNeighbors(df,df2nd,GrainID):\n",
    "        filtr = np.any([df['#grain_i_ID'] == GrainID, df['grain_j_ID'] == GrainID],axis = 0)\n",
    "        grain_j_IDs = list(set(list(df[filtr]['#grain_i_ID'])+list(df[filtr]['grain_j_ID'])))\n",
    "        grain_j_IDs.remove(GrainID)\n",
    "        return list(map(lambda x:df2nd[df2nd['#grainID']==x].index[0],grain_j_IDs))\n",
    "\n",
    "    \n",
    "    @staticmethod    \n",
    "    def GetNeighbors(grain_pairs,GrainID):\n",
    "\n",
    "        filtr = np.any([grain_pairs['#grain_i_ID'] == GrainID, grain_pairs['grain_j_ID'] == GrainID],axis=0)\n",
    "        grain_pairs_filtrd = grain_pairs[ filtr ]\n",
    "\n",
    "        ids = np.c_[grain_pairs_filtrd['#grain_i_ID grain_j_ID'.split()]]\n",
    "        grain_j_IDs = ids[~(ids == GrainID)]\n",
    "\n",
    "        return pd.DataFrame(np.c_[grain_j_IDs,grain_pairs_filtrd['misOrientationAngle(deg) boundaryLength(micron)'.split()]],\n",
    "                            columns='grain_j_ID misOrientationAngle(deg) boundaryLength(micron)'.split())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de913e3e",
   "metadata": {},
   "source": [
    "### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbb84f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open and load data from test_data.csv complete.\n",
      "mkdir: output: File exists\n",
      "GrainIndex= 47 , GrainID= 48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-408-71bf9588e69c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mrho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-408-71bf9588e69c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                                     \u001b[0mGrainID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/boundaryPixels.txt'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                                     \u001b[0mgrain_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                                 )\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-407-508ba99ee4a9>\u001b[0m in \u001b[0;36mParseBoundarySegments\u001b[0;34m(self, GrainID, data, pairs)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisOrientationBoundary\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         list(map(lambda x: neighbors[ neighbors.grain_j_ID == x]['misOrientationAngle(deg)'].iloc[ 0 ]if x != 0.0 else np.nan,\n\u001b[0;32m--> 198\u001b[0;31m         self.neighborIdBoundary))\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-407-508ba99ee4a9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mneighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDislocationDensity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGrainID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisOrientationBoundary\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         list(map(lambda x: neighbors[ neighbors.grain_j_ID == x]['misOrientationAngle(deg)'].iloc[ 0 ]if x != 0.0 else np.nan,\n\u001b[0m\u001b[1;32m    198\u001b[0m         self.neighborIdBoundary))\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3447\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3449\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3502\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3503\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3627\u001b[0m         \"\"\"\n\u001b[0;32m-> 3628\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3629\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m         new_data = self._mgr.take(\n\u001b[0;32m-> 3616\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3617\u001b[0m         )\n\u001b[1;32m   3618\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"take\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         return self.reindex_indexer(\n\u001b[1;32m    866\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             )\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;31m# Note: we discard fill_value and use self._na_value, only relevant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;31m#  in the case where allow_fill is True and fill_value is not None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         taken = algos.take(\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_na_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    data_grains, grain_pairs = load_test_data(\n",
    "                                    confParser['test data files']['test_data_file_path'], \n",
    "                                    confParser['test data files']['test_data_file_path2nd']\n",
    "                                    )\n",
    "    #--- sort based on grain size \n",
    "    GrainIndices   = list(data_grains.sort_values(by='grainSize',ascending=False).index)\n",
    "    \n",
    "    #--- save output\n",
    "    output_dir = confParser['Dislocation Density']['output_dir']\n",
    "    !mkdir $output_dir\n",
    "    fout = '%s/%s'%(output_dir,confParser['Dislocation Density']['fout'])\n",
    "    !rm $fout\n",
    "\n",
    "    \n",
    "    for GrainIndex in GrainIndices:\n",
    "        \n",
    "        #--- grain id\n",
    "        GrainID           =  data_grains.iloc[GrainIndex]['#grainID'].astype(int)\n",
    "        print('GrainIndex=',GrainIndex, ', GrainID=',GrainID)\n",
    "        \n",
    "        #--- dislocation density\n",
    "        cartesian_indices =  confParser['Dislocation Density']['alpha'].split() \n",
    "        #\n",
    "        path              =  confParser['test data files']['ebsd_path']\n",
    "        alpha             =  list(map(lambda x: np.loadtxt('%s/%s.txt'%(path,x)),cartesian_indices))\n",
    "        #\n",
    "        rho               =  DislocationDensity( \n",
    "                                                np.loadtxt('%s/id_matrix.txt'%path), #--- matrix of grain ids\n",
    "                                                np.loadtxt('%s/x_matrix.txt'%path),  #--- xvals\n",
    "                                                np.loadtxt('%s/y_matrix.txt'%path),  #--- yvals   \n",
    "                                                **dict(zip(cartesian_indices,alpha))\n",
    "                                              )\n",
    "        #\n",
    "        rho.Mask( GrainID )\n",
    "        #\n",
    "        rho.ParseBoundarySegments(\n",
    "                                    GrainID,\n",
    "                                    np.loadtxt('%s/boundaryPixels.txt'%path ),\n",
    "                                    grain_pairs,\n",
    "                                )\n",
    "        #\n",
    "        rho.PlotDensityMap(\n",
    "                            GrainID, \n",
    "                            list(data_grains.iloc[GrainIndex]['x y'.split()]),\n",
    "#                             grid              = True,\n",
    "                            boundary_segments = True,\n",
    "                            title             = '%s/densityMap_grainID%s.png'%(output_dir,GrainID),\n",
    "                            colorbar          = True,\n",
    "                            xlabel            = 'x(micron)',\n",
    "                            ylabel            = 'y(micron)',\n",
    "                            )\n",
    "        #\n",
    "        rho.DistanceMatrix( center = list(data_grains.iloc[GrainIndex]['x y'.split()]) )\n",
    "        #        \n",
    "        rho.DensityProfile( \n",
    "                            indx   =  GrainIndex,  \n",
    "                            fout   =  fout,\n",
    "                            plot   =  False,\n",
    "                            scale  =  False,\n",
    "                            xscale =  'log',\n",
    "                            yscale =  'log',\n",
    "#                             xlim   =  (1,10),\n",
    "#                             ylim   =  (0,0.1),\n",
    "                            attrs  =  {'fmt':'x','alpha':0.4},\n",
    "                            xstr   =  'Min. Distance from the boundary (micron)', \n",
    "                            ystr   =  'Dislocation Density',\n",
    "                            title  =  '%s/density_grainID%s.png'%(output_dir,GrainID)\n",
    "                            )\n",
    "#         #\n",
    "#         rho.PrintGrainGrainDensity(  igrain = GrainIndex,\n",
    "#                                         cords = np.c_[test_data_grains['x y'.split()]],\n",
    "#                                         neighbors_i_indices = DislocationDensity.GetNeighbors(test_data_grains2nd,test_data_grains, GrainID = GrainID)\n",
    "                                    \n",
    "#                                     )\n",
    "    return rho\n",
    "\n",
    "rho = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ead36e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GrainID = 24\n",
    "# indics = np.arange(rho.cordxyBoundary.shape[0])\n",
    "\n",
    "# ib = 1111+11\n",
    "# rc=(rho.cordxyBoundary.real[ib],rho.cordxyBoundary.imag[ib])#(-48,-21)\n",
    "# dr=.1\n",
    "# # ibs = indics[np.all([rho.cordxyBoundary.real > rc[0]-dr, \n",
    "# #                     rho.cordxyBoundary.real < rc[0]+dr, \n",
    "# #                     rho.cordxyBoundary.imag > rc[1]-dr,\n",
    "# #                    rho.cordxyBoundary.imag < rc[1]+dr,\n",
    "# #                    ],axis=0)]\n",
    "\n",
    "# ax=utl.PltErr(rho.cordxyBoundary.real,rho.cordxyBoundary.imag,\n",
    "#           attrs={'fmt':'s','ms':1,'color':'brown'},\n",
    "#               Plot=False,\n",
    "#           )\n",
    "\n",
    "# utl.PltErr([rc[0]],[rc[1]],attrs={'fmt':'o','ms':4,'color':'red'},\n",
    "#            ax=ax,\n",
    "#                         Plot=False,\n",
    "\n",
    "#           )\n",
    "\n",
    "\n",
    "# if 1: #for ib in ibs:\n",
    "\n",
    "#     x=rho.pixel_cords_inside.real #rho.pixel_cords_inside[filtr].real\n",
    "#     y=rho.pixel_cords_inside.imag #[filtr].imag\n",
    "    \n",
    "#     utl.PltErr(x,y,attrs={'fmt':'s','ms':2,'color':'black'},Plot=False,ax=ax)\n",
    "# #     utl.PltErr(rho.cordxyBoundary[ib].real,rho.cordxyBoundary[ib].imag,\n",
    "# #                attrs={'fmt':'o','color':'red','ms':10},ax=ax,\n",
    "              \n",
    "\n",
    "#     filtr = rho.nearestBoundarySeg == ib\n",
    "#     print('nearest points=',np.sum(filtr))\n",
    "#     x=rho.pixel_cords_inside[filtr].real\n",
    "#     y=rho.pixel_cords_inside[filtr].imag\n",
    "    \n",
    "#     utl.PltErr(x,y,attrs={'fmt':'^','ms':8,'color':'green'},Plot=False,ax=ax)\n",
    "\n",
    "    \n",
    "# #              )\n",
    "# utl.PltErr(None,None,    \n",
    "#               xlim=(rc[0]-3*dr,rc[0]+3*dr),\n",
    "#               ylim=(rc[1]-3*dr,rc[1]+3*dr),\n",
    "#            ax=ax,\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('len(rho.MinOrientation)',len(rho.MinOrientation))\n",
    "# print('len(rho.misOrientationBoundary)=',len(rho.misOrientationBoundary))\n",
    "# print('neighboring grain id=',rho.neighborIdBoundary[ib])\n",
    "# rho.MinOrientation[filtr]\n",
    "# DislocationDensity.GetNeighbors(grain_pairs,GrainID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905598aa",
   "metadata": {},
   "source": [
    "### AverageDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be1d13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class AverageDensity:\n",
    "    \n",
    "    def __init(self,**kwargs):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def Parse( self, fp, grainIndices ):\n",
    "        rwjs      = utl.ReadWriteJson()\n",
    "        self.data = rwjs.Read( fp )\n",
    "        self.data_concat =\\\n",
    "        np.concatenate(list(map(lambda x:np.c_[x['Distance'],x['Density'],x['Misorientation']]\\\n",
    "        if x['GrainIndex'] in grainIndices else np.c_[np.nan,np.nan,np.nan],self.data)))\n",
    "\n",
    "    def SetFiltr( self, filtr ):\n",
    "        self.filtr       =   filtr\n",
    "\n",
    "    def Average( self, \n",
    "                LogScale = True, \n",
    "                title = 'density.png',\n",
    "                Plot = False,\n",
    "                **kwargs ):\n",
    "        \n",
    "        #--- concat data\n",
    "        Distance         =   self.data_concat[ :, 0 ]\n",
    "        Density          =   self.data_concat[ :, 1 ]\n",
    "        Angles           =   self.data_concat[ :, 2 ]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #--- preprocess\n",
    "\n",
    "        xvals       =   Distance[ self.filtr ]\n",
    "        yvals       =   Density[  self.filtr ]\n",
    "            \n",
    "        if xvals.shape[0] == 0:\n",
    "            return\n",
    "\n",
    "        ymax        =   np.quantile( yvals, 0.95 )\n",
    "        ymin        =   np.quantile( yvals, 0.05 )\n",
    "\n",
    "        if LogScale:\n",
    "            xvals       =   np.log10( xvals )\n",
    "            yvals       =   np.log10( yvals )\n",
    "        \n",
    "        \n",
    "        #--- binning average\n",
    "        xmean, ymean, _, _ = utl.GetBinnedAverage( xvals, yvals,\n",
    "                                                   nbins_per_decade=4,\n",
    "                                                 )\n",
    "#         pdb.set_trace()\n",
    "        if 'median' in kwargs and kwargs['median']:\n",
    "            xmean, ymean   = AverageDensity.GetBinnedMedian( xvals, yvals,\n",
    "                                                       nbins=32,\n",
    "                                                     )\n",
    "            \n",
    "        if LogScale:\n",
    "            xmean = 10**xmean\n",
    "            ymean = 10**ymean\n",
    "            xvals = 10**xvals\n",
    "            yvals = 10**yvals\n",
    "            \n",
    "            \n",
    "        if 'save' in kwargs:\n",
    "            with open(kwargs['save'],'w') as fp:\n",
    "                np.savetxt(fp,np.c_[xmean,ymean])\n",
    "\n",
    "        #--- plot\n",
    "        ax=utl.PltErr( xvals,\n",
    "                       yvals,\n",
    "                       attrs={'fmt':'x','alpha':1},\n",
    "                       Plot=False,\n",
    "                  )\n",
    "        utl.PltErr( xmean, ymean,\n",
    "                    attrs  =  {'fmt':'-o','color':'red',},\n",
    "                    xscale =  'log' if LogScale else 'linear',\n",
    "                    yscale =  'log' if LogScale else 'linear',\n",
    "                    ax     =  ax,\n",
    "#                     ylim   =  (ymin,ymax), #(1e-3,1e-1),#ymin,ymax),\n",
    "#                     xlim   =  (0,10), #(1e-3,1e1),\n",
    "                    xstr   =  'Min. Distance from the boundary (micron)', \n",
    "                    ystr   =  'Dislocation Density',\n",
    "                    title  =  title,\n",
    "                  )\n",
    "\n",
    "    @staticmethod\n",
    "    def GetBinnedMedian(xdata, ydata,\n",
    "                         nbins=10):\n",
    "\n",
    "\n",
    "        xmax = np.max(xdata) + 1.0e-10\n",
    "        xmin = np.min(xdata) - 1.01e-10\n",
    "\n",
    "        xdata_copy = np.copy(xdata)\n",
    "        xdata_copy -= xmin\n",
    "        xdata_copy /= (xmax - xmin)\n",
    "\n",
    "\n",
    "        indices = np.array(list(map(int,xdata_copy * nbins)))\n",
    "\n",
    "        assert np.all(indices < nbins)\n",
    "\n",
    "        #--- group based on index\n",
    "        df = pd.DataFrame(np.c_[indices,xdata,ydata],columns = 'index xvalue yvalue'.split())\n",
    "        sdict = df.groupby(by='index').groups\n",
    "        binnedData = np.concatenate([list(map(lambda x:df.iloc[sdict[x]].median(),sdict))],axis=1)\n",
    "        return binnedData[:,1], binnedData[:,2]\n",
    "\n",
    "    def GetGrainIndices(self,qlo,qhi, path1, path2):    \n",
    "        data_grains, _ = load_test_data(\n",
    "                                        path1, \n",
    "                                        path2\n",
    "                                        )\n",
    "\n",
    "        return AverageDensity.FilterGrains(data_grains,qlo=qlo, qhi=qhi) #--- below median\n",
    "\n",
    "    @staticmethod\n",
    "    def FilterGrains(data_grains,qlo,qhi):\n",
    "\n",
    "    #     GrainIndices   = list(data_grains.sort_values(by='grainSize',ascending=False).index)\n",
    "        size_qlo = np.quantile(data_grains.grainSize,qlo)\n",
    "        size_qhi = np.quantile(data_grains.grainSize,qhi)\n",
    "        filtr  = np.all([data_grains.grainSize >= size_qlo,\n",
    "                         data_grains.grainSize < size_qhi],\n",
    "                         axis=0)\n",
    "        return data_grains[filtr].index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede24338",
   "metadata": {},
   "source": [
    "#### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae369721",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    angles = list(map(float,confParser['Dislocation Density']['angles'].split()))\n",
    "    thetaLoHi  = list(zip(angles,angles[1:]))\n",
    "\n",
    "\n",
    "    output_dir = confParser['Dislocation Density']['output_dir']\n",
    "    finp       = '%s/%s'%(output_dir,confParser['Dislocation Density']['fout'])\n",
    "\n",
    "    #--- lower & upper quantile in terms of size\n",
    "    (qlo, qhi ) = (eval(confParser['Dislocation Density']['qlo']),\\\n",
    "                   eval(confParser['Dislocation Density']['qhi'])\n",
    "                  )\n",
    "    #--- before\n",
    "    count = 0\n",
    "    average_density = AverageDensity( )\n",
    "    average_density.Parse(  finp,\n",
    "                            average_density.GetGrainIndices(qlo,qhi,\n",
    "                            confParser['test data files']['test_data_file_path'],\n",
    "                            confParser['test data files']['test_data_file_path2nd']\n",
    "                                                            )\n",
    "                         )\n",
    "    for (tlo,thi) in thetaLoHi:\n",
    "        average_density.SetFiltr( np.all([  \n",
    "                                            average_density.data_concat[:,1] > 0.0, \n",
    "                                            ~np.isnan(average_density.data_concat[:,2]),\n",
    "                                             average_density.data_concat[:,2] >= tlo, \n",
    "                                             average_density.data_concat[:,2] < thi, \n",
    "                                         ], axis = 0 )\n",
    "                                )\n",
    "\n",
    "        average_density.Average(                        \n",
    "                                LogScale = True,\n",
    "                                median   = True, \n",
    "                                title    = '%s/density_angleIndx%s.png'%(output_dir,count),\n",
    "                                save     = '%s/density_binAveraged_angleIndx%s.txt'%(output_dir,count),\n",
    "                                )\n",
    "        count += 1\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (edges,his,_)=plt.hist(average_density.data_concat[:,2],bins=24)\n",
    "# plt.yscale('log')\n",
    "# plt.ylim(1,1e6)\n",
    "# his[:-1][edges>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04b8ca",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094226c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: png: File exists\r\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if eval(confParser['flags']['remote_machine']):\n",
    "        return\n",
    "    \n",
    "    legend = utl.Legends()\n",
    "    legend.Set()\n",
    "\n",
    "    symbols = utl.Symbols()\n",
    "    !mkdir png\n",
    "    \n",
    "    Alpha = dict(zip(range(100),'alpha12 alpha13 alpha21 alpha23 alpha33'.split()))\n",
    "    Qlo   = dict(zip(range(100),[0.0,0.50]))\n",
    "    Qhi   = dict(zip(range(100),[0.50,1.0]))\n",
    "    irradiate = dict(zip(range(100),['before','after']))\n",
    "    #\n",
    "#    key_alpha0 = 0\n",
    "    key_q0     = 0 #--- condition based on grain size\n",
    "    angleIndx  = 1 #--- condition based on misorientation of boundary segment\n",
    "    #\n",
    "    angles = list(map(float,confParser['Dislocation Density']['angles'].split()))\n",
    "    thetaLoHi  = list(zip(angles,angles[1:]))\n",
    "    output_dir = confParser['Dislocation Density']['output_dir']\n",
    "\n",
    "    #---\n",
    "    for key_alpha in Alpha:\n",
    "        alpha = Alpha[key_alpha]\n",
    "        for key_q in Qlo:\n",
    "            qlo = Qlo[ key_q ]\n",
    "            qhi = Qhi[ key_q ]\n",
    "            if key_q != key_q0:\n",
    "                continue\n",
    "            for (tlo,thi), count in zip(thetaLoHi,range(1000)):\n",
    "                if count != angleIndx:\n",
    "                    continue\n",
    "                ax = utl.PltErr(None,None,Plot=False)\n",
    "                for key_i in irradiate:\n",
    "                    str_irradiate = irradiate[ key_i ]\n",
    "                    path = 'gnd/%s/q%s/alpha%s'%(str_irradiate,key_q,key_alpha)\n",
    "                    try:\n",
    "                        data0 = np.loadtxt('%s/Run0/%s/density_binAveraged_angleIndx%s.txt'\\\n",
    "                                           %(path,output_dir,count))\n",
    "                    except:\n",
    "                        traceback.print_exc()\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    utl.PltErr(data0[:,0],data0[:,1],\n",
    "                                ax     =  ax,\n",
    "                               attrs=symbols.GetAttrs(count=key_i,label=str_irradiate,nevery=2),\n",
    "                               Plot=False,\n",
    "                                )\n",
    "                utl.PltErr(None,None,\n",
    "                            xscale =  'log',\n",
    "                            yscale =  'log',\n",
    "                            ax     =  ax,\n",
    "                            #                         ylim   = (1e-3,1e-1),#ymin,ymax),\n",
    "                            #                     xlim   =  (0,10), #(1e-3,1e1),\n",
    "                            xstr   =  'Min. Distance (micron)', \n",
    "                            ystr   =  alpha, #'Dislocation Density',\n",
    "                            title  =  'png/rho_alpha%s_size%s_angle%s.png'%(key_alpha,key_q,count),\n",
    "                            legend =  legend.Get(),\n",
    "                            dpi    =  50,\n",
    "                            set_title = r'$%s^\\circ~\\langle~\\theta~\\langle~%s^\\circ,%s~\\langle~S~\\langle~%s$'\\\n",
    "                                           %(int(tlo),int(thi),qlo,qhi),\n",
    "                          )\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af087dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_grains, test_data_grains2nd = load_test_data(confParser['test data files']['test_data_file_path'], \n",
    "#                                       confParser['test data files']['test_data_file_path2nd']\n",
    "#                                      )\n",
    "# neighbors_i_indices = DislocationDensity.GetNeighbors(test_data_grains2nd,test_data_grains, GrainID = 24)\n",
    "# print('neighbors of grain id=',GrainID)\n",
    "# display(test_data_grains.iloc[neighbors_i_indices].sort_values(by='grainSize',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11461b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho = PlotGrains( \n",
    "#     np.loadtxt('%s/id_matrix.txt'%confParser['test data files']['ebsd_path']), #--- bitmap of grain ids\n",
    "#     np.loadtxt('%s/alpha12.txt'%confParser['test data files']['ebsd_path']) #--- bitmap of grain ids\n",
    "#     )\n",
    "# rho.GrainGrainDensity(igrain=23,\n",
    "#                         jgrain=28)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
